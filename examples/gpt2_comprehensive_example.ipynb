{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72405b2e",
   "metadata": {},
   "source": [
    "# LLM Interpretability Dashboard - GPT-2 Example\n",
    "\n",
    "This notebook demonstrates the core features of the LLM Interpretability Dashboard using GPT-2 as an example model. We'll explore:\n",
    "\n",
    "1. **Attribution Methods** - Understanding which tokens are important\n",
    "2. **Activation Patching** - Testing causal relationships\n",
    "3. **Attention Analysis** - Visualizing attention patterns\n",
    "4. **Mechanistic Analysis** - Logit lens and neuron analysis\n",
    "5. **Visualization** - Creating publication-ready plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de86fd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and load a GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e26b1ce0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Import our interpretability library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from interpret_llm import load_model_and_tokenizer, get_config\n",
    "from interpret_llm.attribution import GradientAttributor, AttentionAttributor, AttributionVisualizer\n",
    "from interpret_llm.patching import ActivationPatcher, CausalTracer\n",
    "from interpret_llm.circuits import LogitLens, NeuronAnalyzer, AttentionHeadAblator\n",
    "from interpret_llm.visualization import TextOverlayVisualizer, HeatmapVisualizer\n",
    "\n",
    "# Set up device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, device=device)\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f231a3e",
   "metadata": {},
   "source": [
    "## 1. Attribution Analysis\n",
    "\n",
    "Let's start by understanding which tokens are important for the model's predictions using various attribution methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75113f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text for analysis\n",
    "text = \"The Eiffel Tower is located in Paris, the capital of France.\"\n",
    "\n",
    "print(f\"Analyzing text: '{text}'\")\n",
    "print(f\"Tokens: {tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d540a33",
   "metadata": {},
   "source": [
    "### Gradient-based Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize gradient attributor\n",
    "grad_attributor = GradientAttributor(model, tokenizer, device)\n",
    "\n",
    "# Compute different attribution methods\n",
    "attribution_results = grad_attributor.compute_all_attributions(\n",
    "    text,\n",
    "    methods=[\"vanilla_gradient\", \"gradient_x_input\", \"integrated_gradients\"]\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for method_name, result in attribution_results.items():\n",
    "    print(f\"\\n{method_name.upper()}:\")\n",
    "    print(f\"Target token: {result.metadata.get('target_token_id', 'N/A')}\")\n",
    "    print(f\"Target score: {result.metadata.get('target_score', 'N/A'):.4f}\")\n",
    "    \n",
    "    # Show top attributed tokens\n",
    "    top_indices = torch.argsort(result.attributions, descending=True)[:5]\n",
    "    print(\"Top attributed tokens:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        token = result.tokens[idx]\n",
    "        score = result.attributions[idx].item()\n",
    "        print(f\"  {i+1}. '{token}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d9b02",
   "metadata": {},
   "source": [
    "### Attention-based Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d15522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize attention attributor\n",
    "attn_attributor = AttentionAttributor(model, tokenizer, device)\n",
    "\n",
    "# Compute attention rollout\n",
    "rollout_result = attn_attributor.attention_rollout(text)\n",
    "\n",
    "print(\"ATTENTION ROLLOUT:\")\n",
    "print(f\"Sequence length: {len(rollout_result.tokens)}\")\n",
    "\n",
    "# Show top attributed tokens\n",
    "top_indices = torch.argsort(rollout_result.attributions, descending=True)[:5]\n",
    "print(\"Top attributed tokens:\")\n",
    "for i, idx in enumerate(top_indices):\n",
    "    token = rollout_result.tokens[idx]\n",
    "    score = rollout_result.attributions[idx].item()\n",
    "    print(f\"  {i+1}. '{token}': {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1873028",
   "metadata": {},
   "source": [
    "### Visualizing Attribution Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f1b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizers\n",
    "attr_viz = AttributionVisualizer()\n",
    "text_viz = TextOverlayVisualizer()\n",
    "\n",
    "# Create token heatmap for integrated gradients\n",
    "ig_result = attribution_results[\"integrated_gradients\"]\n",
    "fig = attr_viz.plot_token_heatmap(\n",
    "    ig_result,\n",
    "    title=\"Integrated Gradients Attribution\",\n",
    "    show_values=True\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Create comparison plot\n",
    "comparison_fig = attr_viz.compare_attributions(\n",
    "    attribution_results,\n",
    "    title=\"Attribution Method Comparison\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eafe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive HTML visualization\n",
    "html_viz = text_viz.create_html_overlay(\n",
    "    ig_result.tokens,\n",
    "    ig_result.attributions,\n",
    "    title=\"Interactive Token Attribution\",\n",
    "    save_path=\"attribution_visualization.html\"\n",
    ")\n",
    "\n",
    "print(\"Interactive HTML visualization saved to 'attribution_visualization.html'\")\n",
    "print(\"Open this file in a web browser to see the interactive visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2742028c",
   "metadata": {},
   "source": [
    "## 2. Activation Patching\n",
    "\n",
    "Now let's explore causal relationships by patching activations at different components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb23bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize activation patcher\n",
    "with ActivationPatcher(model, tokenizer, device) as patcher:\n",
    "    \n",
    "    # Run systematic ablation on a few layers\n",
    "    ablation_results = patcher.run_systematic_ablation(\n",
    "        text,\n",
    "        layers=[6, 7, 8, 9, 10, 11],  # Focus on later layers\n",
    "        components=[\"attention\", \"mlp\"],\n",
    "        heads=[0, 1, 2, 3, 4, 5]  # First few heads\n",
    "    )\n",
    "    \n",
    "    print(f\"Ran {len(ablation_results)} ablation experiments\")\n",
    "    \n",
    "    # Find most critical components\n",
    "    critical_components = patcher.find_critical_components(\n",
    "        text,\n",
    "        metric=\"logit_l2_diff\",\n",
    "        threshold=0.1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFound {len(critical_components)} critical components:\")\n",
    "    for (location, impact), result in critical_components[:10]:\n",
    "        print(f\"  {location.layer_idx}_{location.component}\", end=\"\")\n",
    "        if location.head_idx is not None:\n",
    "            print(f\"_H{location.head_idx}\", end=\"\")\n",
    "        print(f\": {impact:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9163ec1",
   "metadata": {},
   "source": [
    "### Causal Tracing\n",
    "\n",
    "Let's perform causal tracing to understand information flow for factual associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eabdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize causal tracer\n",
    "with CausalTracer(model, tokenizer, device) as tracer:\n",
    "    \n",
    "    # Trace causal effect for \"Paris\" given \"Eiffel Tower\"\n",
    "    trace_result = tracer.trace_causal_effect(\n",
    "        text=\"The Eiffel Tower is located in\",\n",
    "        subject_tokens=[\"Eiffel\", \"Tower\"],\n",
    "        target_token_position=-1  # Last position\n",
    "    )\n",
    "    \n",
    "    print(\"CAUSAL TRACING RESULTS:\")\n",
    "    print(f\"Baseline score: {trace_result.baseline_score:.4f}\")\n",
    "    print(f\"Corrupted score: {trace_result.corrupted_score:.4f}\")\n",
    "    print(f\"Subject positions: {trace_result.subject_token_positions}\")\n",
    "    \n",
    "    # Show top restoration effects\n",
    "    sorted_effects = sorted(\n",
    "        trace_result.trace_results.items(),\n",
    "        key=lambda x: x[1][\"restoration_effect\"],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTop restoration effects:\")\n",
    "    for component, metrics in sorted_effects[:10]:\n",
    "        effect = metrics[\"restoration_effect\"]\n",
    "        print(f\"  {component}: {effect:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539187b",
   "metadata": {},
   "source": [
    "## 3. Attention Analysis\n",
    "\n",
    "Let's visualize attention patterns to understand how the model processes information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da991f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights for visualization\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "attentions = outputs.attentions  # List of attention weights per layer\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "print(f\"Got attention weights for {len(attentions)} layers\")\n",
    "print(f\"Attention shape per layer: {attentions[0].shape}\")  # [batch, heads, seq, seq]\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be74ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "heatmap_viz = HeatmapVisualizer()\n",
    "\n",
    "# Show attention heatmap for last layer, first head\n",
    "last_layer_attention = attentions[-1][0]  # Remove batch dimension\n",
    "\n",
    "fig = heatmap_viz.attention_heatmap(\n",
    "    last_layer_attention[0],  # First head\n",
    "    tokens,\n",
    "    title=\"Attention Pattern - Last Layer, Head 0\",\n",
    "    layer_idx=len(attentions)-1,\n",
    "    head_idx=0,\n",
    "    show_values=False\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Show multi-head attention grid for layer 8\n",
    "layer_8_attention = attentions[8][0]  # Remove batch dimension\n",
    "\n",
    "fig = heatmap_viz.multi_head_attention_grid(\n",
    "    layer_8_attention,\n",
    "    tokens,\n",
    "    title=\"Multi-Head Attention\",\n",
    "    layer_idx=8,\n",
    "    max_heads=8\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f7ab0",
   "metadata": {},
   "source": [
    "## 4. Mechanistic Analysis\n",
    "\n",
    "Now let's dive deeper into the model's internal mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f4ab0",
   "metadata": {},
   "source": [
    "### Logit Lens Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ee3d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logit lens\n",
    "logit_lens = LogitLens(model, tokenizer, device)\n",
    "\n",
    "# Analyze layer-by-layer predictions\n",
    "lens_result = logit_lens.analyze(\n",
    "    text,\n",
    "    layers=[0, 2, 4, 6, 8, 10, 11],  # Sample of layers\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"LOGIT LENS ANALYSIS:\")\n",
    "print(f\"Analyzed {len(lens_result.layer_predictions)} layers\")\n",
    "print(f\"Input tokens: {lens_result.tokens}\")\n",
    "\n",
    "# Show predictions for the last token position across layers\n",
    "last_position = len(lens_result.tokens) - 1\n",
    "print(f\"\\nPredictions for position {last_position} ('{lens_result.tokens[last_position]}'):\")\n",
    "\n",
    "for layer_idx in sorted(lens_result.layer_predictions.keys()):\n",
    "    top_tokens = lens_result.top_tokens[layer_idx][last_position]\n",
    "    print(f\"  Layer {layer_idx}: {top_tokens[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b586c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction convergence\n",
    "convergence = logit_lens.analyze_convergence(\n",
    "    lens_result,\n",
    "    position=last_position\n",
    ")\n",
    "\n",
    "print(\"CONVERGENCE ANALYSIS:\")\n",
    "print(f\"Final prediction: '{convergence.get('final_prediction')}'\")\n",
    "print(f\"Convergence layer: {convergence.get('convergence_layer')}\")\n",
    "\n",
    "# Plot convergence probabilities\n",
    "if 'convergence_probabilities' in convergence:\n",
    "    probs = convergence['convergence_probabilities']\n",
    "    layers = convergence['layers_analyzed']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(layers, probs, 'o-', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Probability of Final Prediction')\n",
    "    plt.title('Prediction Convergence Across Layers')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384b1d9",
   "metadata": {},
   "source": [
    "### Attention Head Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babe1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize head ablator\n",
    "with AttentionHeadAblator(model, tokenizer, device) as ablator:\n",
    "    \n",
    "    # Systematically ablate attention heads in later layers\n",
    "    head_results = ablator.systematic_head_ablation(\n",
    "        text,\n",
    "        layers=[8, 9, 10, 11],  # Later layers\n",
    "        heads=[0, 1, 2, 3, 4, 5, 6, 7],  # First 8 heads\n",
    "        ablation_type=\"zero\"\n",
    "    )\n",
    "    \n",
    "    print(\"HEAD ABLATION RESULTS:\")\n",
    "    print(f\"Baseline score: {head_results.baseline_score:.4f}\")\n",
    "    print(f\"Tested {len(head_results.head_impacts)} heads\")\n",
    "    \n",
    "    # Find critical heads\n",
    "    critical_heads = ablator.find_critical_heads(\n",
    "        head_results,\n",
    "        threshold=0.05,\n",
    "        top_k=10\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMost critical heads:\")\n",
    "    for head_id, impact in critical_heads:\n",
    "        print(f\"  {head_id}: {impact:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of head impacts\n",
    "heatmap_matrix = ablator.layer_head_heatmap(head_results)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(heatmap_matrix, cmap='Reds', aspect='auto')\n",
    "plt.colorbar(label='Ablation Impact')\n",
    "plt.xlabel('Head Index')\n",
    "plt.ylabel('Layer Index')\n",
    "plt.title('Attention Head Ablation Impact Heatmap')\n",
    "\n",
    "# Add text annotations for high-impact heads\n",
    "for i in range(heatmap_matrix.shape[0]):\n",
    "    for j in range(heatmap_matrix.shape[1]):\n",
    "        if heatmap_matrix[i, j] > 0.1:  # Only annotate significant impacts\n",
    "            plt.text(j, i, f'{heatmap_matrix[i, j]:.2f}', \n",
    "                    ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb0c44",
   "metadata": {},
   "source": [
    "### Neuron Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3097afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize neuron analyzer\n",
    "neuron_analyzer = NeuronAnalyzer(model, tokenizer, device)\n",
    "\n",
    "# Extract neuron activations for MLP layers\n",
    "neuron_results = neuron_analyzer.extract_neuron_activations(\n",
    "    text,\n",
    "    components=[\"mlp\"],\n",
    "    layers=[8, 9, 10]  # Focus on later layers\n",
    ")\n",
    "\n",
    "print(\"NEURON ANALYSIS:\")\n",
    "print(f\"Components analyzed: {list(neuron_results.neuron_activations.keys())}\")\n",
    "\n",
    "# Show statistics for some neurons\n",
    "print(\"\\nSample neuron statistics:\")\n",
    "for neuron_id, stats in list(neuron_results.activation_statistics.items())[:5]:\n",
    "    print(f\"  {neuron_id}:\")\n",
    "    print(f\"    Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "    print(f\"    Max: {stats['max']:.4f}, Sparsity: {stats['sparsity']:.4f}\")\n",
    "    \n",
    "    # Show max activating tokens\n",
    "    if neuron_id in neuron_results.max_activating_tokens:\n",
    "        max_tokens = neuron_results.max_activating_tokens[neuron_id][:3]\n",
    "        print(f\"    Top tokens: {max_tokens}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42519ca6",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Analysis Dashboard\n",
    "\n",
    "Let's create a comprehensive analysis dashboard that combines multiple interpretability methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interpretability_dashboard(text, model, tokenizer, device):\n",
    "    \"\"\"Create a comprehensive interpretability dashboard.\"\"\"\n",
    "    \n",
    "    print(f\"üîç INTERPRETABILITY DASHBOARD\")\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Tokens: {tokenizer.tokenize(text)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Attribution Analysis\n",
    "    print(\"\\nüìä ATTRIBUTION ANALYSIS\")\n",
    "    grad_attributor = GradientAttributor(model, tokenizer, device)\n",
    "    \n",
    "    # Quick attribution\n",
    "    ig_result = grad_attributor.integrated_gradients(text)\n",
    "    top_idx = torch.argmax(ig_result.attributions)\n",
    "    top_token = ig_result.tokens[top_idx]\n",
    "    top_score = ig_result.attributions[top_idx].item()\n",
    "    \n",
    "    print(f\"Most important token: '{top_token}' (score: {top_score:.4f})\")\n",
    "    \n",
    "    # 2. Causal Analysis\n",
    "    print(\"\\nüîó CAUSAL ANALYSIS\")\n",
    "    with ActivationPatcher(model, tokenizer, device) as patcher:\n",
    "        critical_components = patcher.find_critical_components(\n",
    "            text, threshold=0.05\n",
    "        )\n",
    "        \n",
    "        if critical_components:\n",
    "            top_component = critical_components[0]\n",
    "            location, impact = top_component\n",
    "            comp_name = f\"Layer {location.layer_idx}, {location.component}\"\n",
    "            if location.head_idx is not None:\n",
    "                comp_name += f\", Head {location.head_idx}\"\n",
    "            print(f\"Most critical component: {comp_name} (impact: {impact:.4f})\")\n",
    "        else:\n",
    "            print(\"No critical components found above threshold\")\n",
    "    \n",
    "    # 3. Attention Analysis\n",
    "    print(\"\\nüëÅÔ∏è ATTENTION ANALYSIS\")\n",
    "    attn_attributor = AttentionAttributor(model, tokenizer, device)\n",
    "    rollout_result = attn_attributor.attention_rollout(text)\n",
    "    \n",
    "    top_attn_idx = torch.argmax(rollout_result.attributions)\n",
    "    top_attn_token = rollout_result.tokens[top_attn_idx]\n",
    "    top_attn_score = rollout_result.attributions[top_attn_idx].item()\n",
    "    \n",
    "    print(f\"Most attended token: '{top_attn_token}' (score: {top_attn_score:.4f})\")\n",
    "    \n",
    "    # 4. Mechanistic Analysis\n",
    "    print(\"\\nüî¨ MECHANISTIC ANALYSIS\")\n",
    "    logit_lens = LogitLens(model, tokenizer, device)\n",
    "    lens_result = logit_lens.analyze(text, layers=[0, 6, 11])\n",
    "    \n",
    "    last_pos = len(lens_result.tokens) - 1\n",
    "    final_prediction = lens_result.top_tokens[11][last_pos][0] if 11 in lens_result.top_tokens else \"N/A\"\n",
    "    early_prediction = lens_result.top_tokens[0][last_pos][0] if 0 in lens_result.top_tokens else \"N/A\"\n",
    "    \n",
    "    print(f\"Early prediction (Layer 0): '{early_prediction}'\")\n",
    "    print(f\"Final prediction (Layer 11): '{final_prediction}'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚úÖ Dashboard complete!\")\n",
    "\n",
    "# Run the dashboard\n",
    "create_interpretability_dashboard(text, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc3caf",
   "metadata": {},
   "source": [
    "## 6. Comparison Across Different Inputs\n",
    "\n",
    "Let's compare how the model processes different types of factual statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f9c022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different factual statements\n",
    "test_texts = [\n",
    "    \"The Eiffel Tower is located in Paris, the capital of France.\",\n",
    "    \"Albert Einstein developed the theory of relativity.\", \n",
    "    \"The Amazon River flows through Brazil.\",\n",
    "    \"William Shakespeare wrote Romeo and Juliet.\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ COMPARATIVE ANALYSIS\")\n",
    "print(\"Analyzing different factual statements...\\n\")\n",
    "\n",
    "# Quick analysis for each text\n",
    "for i, test_text in enumerate(test_texts, 1):\n",
    "    print(f\"{i}. {test_text}\")\n",
    "    \n",
    "    # Attribution analysis\n",
    "    grad_attributor = GradientAttributor(model, tokenizer, device)\n",
    "    ig_result = grad_attributor.integrated_gradients(test_text)\n",
    "    \n",
    "    # Find most important token\n",
    "    top_idx = torch.argmax(ig_result.attributions)\n",
    "    top_token = ig_result.tokens[top_idx]\n",
    "    top_score = ig_result.attributions[top_idx].item()\n",
    "    \n",
    "    print(f\"   Most important token: '{top_token}' (score: {top_score:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a030fd",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the core capabilities of the LLM Interpretability Dashboard:\n",
    "\n",
    "### ‚úÖ What we covered:\n",
    "1. **Attribution Methods**: Gradient-based and attention-based attribution\n",
    "2. **Activation Patching**: Systematic ablation and causal tracing\n",
    "3. **Attention Analysis**: Visualizing attention patterns\n",
    "4. **Mechanistic Analysis**: Logit lens, neuron analysis, and head ablation\n",
    "5. **Visualization**: Rich plots and interactive visualizations\n",
    "\n",
    "### üöÄ Next steps:\n",
    "1. Try with larger models (GPT-2 medium/large, GPT-Neo, LLaMA)\n",
    "2. Analyze more complex tasks (reasoning, factual recall, sentiment)\n",
    "3. Explore neuron clustering and circuit discovery\n",
    "4. Build custom attribution methods\n",
    "5. Create publication-ready visualizations\n",
    "\n",
    "### üìö Resources:\n",
    "- [Full documentation](../README.md)\n",
    "- [API reference](../docs/api.md)\n",
    "- [Additional examples](../examples/)\n",
    "\n",
    "Happy interpreting! üß†‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpret-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
